{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity: Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L4: Text and Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a text embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words are really hard - there are lots of them, and most of them we never see. But usually it is the rarest words that provide the most information. This is a big problem in deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Ambiguity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to share parameters between things that are similar, e.g. \"cat\" and \"kitty.\" But these words are not always similar!\n",
    "\n",
    "Big problems:\n",
    "\n",
    "1. we'd like to see the really rare words often enough that we can learn how to use them\n",
    "\n",
    "2. we need a way to relate words that are similar in semantic meaning\n",
    "\n",
    "We need lots of labeled data to do this - more labeled data than we can handle! What to do..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised learning means training without labels. There is lots and lots of text out there (e.g., the web) _if_ we can figure out what to learn from it. There is a powerful concept we may exploit - similar words appear in similar contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \"the **cat** purrs\"\n",
    "* \"this **cat** hunts mice\"\n",
    "\n",
    "Also reasonable to say\n",
    "\n",
    "* \"the **kitty** purrs\"\n",
    "* \"this **kitty** hunts mice\"\n",
    "\n",
    "<img src='Screen_Shot_2016-03-01_at_6.10.15_PM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hope we can produce a model that predicts a word's context. A model that predicts context will have to treat \"cat\" and \"kitty\" similarly and tend to bring them closer together.\n",
    "\n",
    "<img src='Screen_Shot_2016-03-01_at_6.12.13_PM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helps us solve the sparsity and representation problem once we have encoded our words into vectors called _embeddings_. Now all cat-like things will be similar, etc. The model no longer has to learn new things for every way there is to use a word - we get _generalization_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "<img src='Screen_Shot_2016-03-01_at_6.16.21_PM.png'>\n",
    "\n",
    "1. We map each word in the sentance into an embedding - initially a random one.\n",
    "2. We will use the embedding to try to predict the context of the word.\n",
    "3. In this model, the context is the words that are nearby (within a window). We choose a random word in a window around our original word, and that becomes our target.\n",
    "4. Then, we train our model as though it were a supervised problem. We use _logistic classifiers_ to make our prediction.\n",
    "\n",
    "This is the _skip-gram_ model - we predict the context from the word. In the _continuous bag of words (CBOW)_ model, we average (? sum?) over the context to predict the target. So these approaches are two sides of the same coin - either the context is the label we want to predict from the chosen word, or the context is the data we want to use to select a target word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be good to see that our training was working by clustering similar words closer together. One way to do this would be to do a nearest neighbors look-up. Another way would be to reduce the dimensionality of the representation and project into two dimensions. But we need a way of doing this that preserves the neighborhood structure (things that are close in embedding space should still be close in 2D, and things that are far should remain far). This means PCA ends up not working so well, but a technique called \"t-SNE\" works well.\n",
    "\n",
    "<img src='Screen_Shot_2016-03-01_at_6.24.07_PM.png'>\n",
    "\n",
    "<img src='Screen_Shot_2016-03-01_at_6.25.40_PM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Screen_Shot_2016-03-01_at_6.26.33_PM.png'>\n",
    "\n",
    "The cosine distance is generally a better measure than $L_2$ - this because the length of the embedding vector is not relevant to the classification. In fact, it is often better to normalize all embedding vectors to have unit norm.\n",
    "\n",
    "We can also use _sampled softmax_ to make our computations easier:\n",
    "\n",
    "<img src='Screen_Shot_2016-03-01_at_6.29.29_PM.png'>\n",
    "\n",
    "The idea behind sampled softmax is rather than treat our target vector such that the target word has probability 1 and every other word has probability 0, we only sample a small random subset of the whole rest of the vocabulary, and pretend the other words aren't there. This makes computations much more efficient and is almost as good as using the full vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Word Analogy Game Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Screen_Shot_2016-03-04_at_7.52.57_AM.png'>\n",
    "\n",
    "* kitten\n",
    "* shorter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saying a puppy is to a dog as what a kitten is to a cat is an example of a _semnatic analogy_. Saying taller is to tall as shorter is to short is an example of a _syntactic analogy_.\n",
    "\n",
    "<img src='Screen_Shot_2016-03-04_at_7.59.43_AM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segue to Assignment 5: Word2Vec and CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Word2Vec and use it as a refernce to train a continuous bag of words (CBOW) model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Sequences of Varying Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we deal with models of sequences of words (as opposed to just individual words)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Screen_Shot_2016-05-03_at_6.18.15_PM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our sequence is reasonably \"stationary,\" we can use the same model at each point in time (share weights across time rather than space). But, because it is a sequence, we want to also take into account the past. A natural way to do this is to use the previous state of the classifier as a summary, recursively. We can make this efficient by using a single model to take into account the past (rather than having many layers going back hundreds or thousands of layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Screen_Shot_2016-05-03_at_6.22.45_PM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a network with a relatively simple repeating pattern:\n",
    "\n",
    "<img src='Screen_Shot_2016-05-03_at_6.23.18_PM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backprop through time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to back-propagate updates all the way back through time, to the very beginning of the sequence. In practice, we go as deep as we can (going to \"the beginning\" is usually computationally prohibitive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All these derivatives are applied to the same parameters. This is bad for SGD, which prefers uncorrelated updates.\n",
    "\n",
    "<img src='Screen_Shot_2016-05-03_at_6.26.20_PM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes the math very unstable:\n",
    "\n",
    "<img src='Screen_Shot_2016-05-03_at_6.27.35_PM.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
